<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>[PRML] Chapter4 - Linear Models For Classification (1) - minsoo9506</title><meta name="Description" content="This is My New Hugo Site"><meta property="og:title" content="[PRML] Chapter4 - Linear Models For Classification (1)" />
<meta property="og:description" content="Classification에 대해 알아보자." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://minsoo9506.github.io/prml-chap04-1/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-11-26T17:25:26+09:00" />
<meta property="article:modified_time" content="2021-11-26T17:25:26+09:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="[PRML] Chapter4 - Linear Models For Classification (1)"/>
<meta name="twitter:description" content="Classification에 대해 알아보자."/>
<meta name="application-name" content="minsoo9506">
<meta name="apple-mobile-web-app-title" content="minsoo9506"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="http://minsoo9506.github.io/prml-chap04-1/" /><link rel="prev" href="http://minsoo9506.github.io/prml-chap03-2/" /><link rel="next" href="http://minsoo9506.github.io/prml-chap04-2/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "[PRML] Chapter4 - Linear Models For Classification (1)",
        "inLanguage": "en",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "http:\/\/minsoo9506.github.io\/prml-chap04-1\/"
        },"genre": "posts","keywords": "Classification","wordcount":  1591 ,
        "url": "http:\/\/minsoo9506.github.io\/prml-chap04-1\/","datePublished": "2021-11-26T17:25:26+09:00","dateModified": "2021-11-26T17:25:26+09:00","publisher": {
            "@type": "Organization",
            "name": "minsoo9506"},"author": {
                "@type": "Person",
                "name": "minsoo9506"
            },"description": ""
    }
    </script></head>
    <body header-desktop="auto" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="minsoo9506">minsoo9506 study note</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="minsoo9506">minsoo9506 study note</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw"></i>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animated flipInX">[PRML] Chapter4 - Linear Models For Classification (1)</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://github.com/minsoo9506" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw"></i>minsoo9506</a></span>&nbsp;<span class="post-category">included in <a href="/categories/prml/"><i class="far fa-folder fa-fw"></i>PRML</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2021-11-26">2021-11-26</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;1591 words&nbsp;
                <i class="far fa-clock fa-fw"></i>&nbsp;8 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  kept="true">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#41-discriminant-functions">4.1 Discriminant Functions</a>
      <ul>
        <li><a href="#411-two-classes">4.1.1 two classes</a></li>
        <li><a href="#412-multiple-classes">4.1.2 Multiple classes</a></li>
        <li><a href="#413-least-squares-for-classification">4.1.3 Least squares for classification</a></li>
        <li><a href="#414-fishers-linear-discriminant">4.1.4 Fisher&rsquo;s linear discriminant</a></li>
        <li><a href="#415-relation-to-least-squares">4.1.5 Relation to least squares</a></li>
        <li><a href="#416-fishers-discriminant-for-multiple-classes">4.1.6 Fisher&rsquo;s discriminant for multiple classes</a></li>
        <li><a href="#417-the-perceptron-algorithm">4.1.7 The perceptron algorithm</a></li>
      </ul>
    </li>
    <li><a href="#42-probabilistic-generative-models">4.2 Probabilistic Generative Models</a>
      <ul>
        <li><a href="#421-continuous-inputs">4.2.1 Continuous inputs</a></li>
        <li><a href="#422-maximum-likelihood-solution">4.2.2 Maximum likelihood solution</a></li>
        <li><a href="#423-discrete-features">4.2.3 Discrete features</a></li>
        <li><a href="#424-exponential-family">4.2.4 Exponential Family</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><p>Classification에 대해 알아보자.</p>
<p>input space는 <em>decision regions</em> 로 나눠지는데 이는 <em>decision boundaries(decision surfaces)</em> 에 의해 나눠진다. 이번 챕터에서는 분류 선형모델에 대해 공부하는데  이는 decision surfaces가 <strong>input x의 linear function</strong> 이라는 것을 의미한다. <strong>D차원의 input space가 D-1 차원의 hyperplane으로 나눠지는 것이다.</strong> 크게 3가지로 나누어서 공부한다.</p>
<ul>
<li>Discriminant function</li>
<li>generative</li>
<li>Discriminative</li>
</ul>
<p>classification에서는 discrete class labels 이나 각 class가 될 probability를 target으로 예측한다. 후자의 경우 (0,1) 사이의 값을 가질 것이다. 따라서 우리는 linear function of ${\bf w}$를 nonlinear function을 이용하여 transform한다.</p>
<p>$$y({\bf x}) = f({\bf w}^T {\bf x} + w_0)$$</p>
<p>machine learning에서는 $f$를 <em>activation function</em> 이라고 부른다. 통계학에서는 inverse of <em>link function</em> 으로 부른다. 따라서 이전에 봤던 regression model과는 다르게 더이상 parameter에 linear하지 않는 성질을 가진다.</p>
<h2 id="41-discriminant-functions">4.1 Discriminant Functions</h2>
<ul>
<li>discriminant : a function that takes an input vector ${\bf x}$ and assigns it to one of $K$ class
<ul>
<li>이번 chapter에서는 <em>linear discriminant</em> ( : decision surfaces are hyperplane) 로 한정지어 공부할 것이다.</li>
</ul>
</li>
</ul>
<h3 id="411-two-classes">4.1.1 two classes</h3>
<p>가장 간단한 linear discriminant function을 보면</p>
<p>$$y({\bf x}) = {\bf w}^T {\bf x} + w_0$$</p>
<ul>
<li>$y({\bf x}) \ge 0$ 이면 class 1이고 반대면 class 2 이다. 따라서 decision boundary는 $y({\bf x}) = 0$ 이고 $(D-1)$차원의 hyperplane이다.</li>
<li>decision surface 위에 두 점 ${\bf x}_A , {\bf x}_B$ 이 있다고 가정하면
<ul>
<li>${\bf w}^T({\bf x}_A - {\bf x}_B)=0$ 이므로 vector ${\bf w}$는 decision surface에 있는 모든 점들과 orthogonal하다. 이는 ${\bf w}$가 decision surface의 orientation을 결정한다는 의미이다.</li>
</ul>
</li>
<li>똑같이 ${\bf x}$가 decision surface 위의 점이라고 하고 원점과 decision surface의 거리를 계산하면 아래와 같다.
<ul>
<li>여기서 ${\bf w}_0$는 decision surface의 위치를 결정한다.</li>
</ul>
</li>
</ul>
<p>$$\frac{\textbf{w}^T \textbf{x}}{\left|| \textbf{w} \right||} = - \frac{ \textbf{w}_0}{\left|| \textbf{w} \right||}$$</p>
<h3 id="412-multiple-classes">4.1.2 Multiple classes</h3>
<p>$K$ 가 2보다 큰 multiple class를 분류하는 상황을 생각해보자. linear discriminant로 분류하는 방법은 크게 두 가지로 나눌 수 있다.</p>
<ul>
<li>one vs the rest</li>
<li>one vs one</li>
</ul>
<p>두 방법 모두 class를 결정하는데 있어 애매한 상황이 발생한다. hyperplane이라는 제약때문에 그 어떤 class에도 속하지 못하는 지역이 발생한다. (PRML figure 4.2 에 잘 보여줌) 이를 해결하기 위해 아래와 같은 $K$개의 linear function을 $K$-class discriminant로 사용한다.</p>
<p>$$y_k(x) = w^T_kx + w_{k0}$$</p>
<ul>
<li>$y_k({\bf x}) \ge y_j ({\bf x})$ 인 경우, ${\bf x}$는 $k$로 분류한다. 즉, 큰 값을 가지는 쪽으로!</li>
</ul>
<p>여기서 만들어지는 decision region은 항상 singly connected and convex하다.</p>
<ul>
<li>decision region $R_k$에 들어있는 두 점 ${\bf x}_A, {\bf x}_B$</li>
<li>이 두 점을 연결한 선 위에 점 $\hat{ {\bf x} }$이 있다고 가정하자. 이를 표현하면 ($0 \le \lambda \le 1$)</li>
</ul>
<p>$$\hat{\bf x}=\lambda{\bf x}_A + (1-\lambda){\bf x}_B$$</p>
<p>따라서 discriminant function은 다음을 만족한다.</p>
<p>$$y_k(\hat{ {\bf x} })={\lambda}y_k({\bf x}_A) + (1-\lambda)y_k({\bf x}_B) $$</p>
<ul>
<li>$y_k({\bf x}_A)  &gt; y_j({\bf x}_A) , y_k({\bf x}_B)  &gt; y_j({\bf x}_B)$ 을 만족하기에</li>
<li>$y_k({\hat {\bf x}})  &gt; y_j({\hat {\bf x}})$ 도 성립한다.
<ul>
<li>따라서, ${\hat {\bf x}}$은 항상 $R_k$에 속한다.</li>
</ul>
</li>
</ul>
<p>이제 linear discriminant function의 parameter를 학습하는 방법에 대해 배울 것이다.</p>
<ul>
<li>least square</li>
<li>Fisher&rsquo;s linear discriminant</li>
<li>perceptron algorithm</li>
</ul>
<h3 id="413-least-squares-for-classification">4.1.3 Least squares for classification</h3>
<p>이전의 sum of squares error function을 그대로 이용한다. target은 1-of-K binary coding하여 vector ${\bf t}$ 이다. (해당하는 class는 1 나머지 class는 0으로 표현)</p>
<ul>
<li>각 class 마다 $y_k({\bf x}) = {\bf w}_k^T {\bf x} + w _ {k0}$ , 이를 합쳐서 표현하면</li>
</ul>
<p>$$\textbf{y} (\textbf{x}) = \widetilde{ \textbf{W} }^T \widetilde{ {\bf x} }$$</p>
<ul>
<li>$\widetilde{\textbf{W}}$ : 각 컬럼이 $\widetilde{\textbf{w}}_k = ({\bf w} _ {k0}, {\bf w}_k^T)$</li>
<li>$\widetilde{\textbf{W}}_k^T \widetilde{ {\bf x}}$가 가장 큰 값(class)에 input ${\bf x}$를 할당한다.</li>
<li>normal equation으로 parameter를 구하면</li>
</ul>
<p>$$\widetilde{\textbf{W}}=(\widetilde{\textbf{W}}^T \widetilde{\textbf{W}})^{-1}\widetilde{\textbf{W}}^T\widetilde{\textbf{T}}=\widetilde{\textbf{W}}^{\dagger}\widetilde{\textbf{T}}$$</p>
<ul>
<li>특징
<ul>
<li>exact closed-form의 solution이 나온다.</li>
<li>output이 확률의 범위 (0,1) 을 넘어가는 경우가 존재한다. (우리는 output이 확률값이길 원한다)</li>
<li>least square의 단점인 outlier에 취약하다. input data에 따라서 decision이 급변하다.</li>
</ul>
</li>
</ul>
<h3 id="414-fishers-linear-discriminant">4.1.4 Fisher&rsquo;s linear discriminant</h3>
<p>차원 축소의 역할로 많이 쓰이는데 classification으로도 사용가능하다. 일단은 2-class의 경우만 고려해보자.</p>
<ul>
<li>$D$ 차원의 input vector $\textbf{W}$를 1차원에 project한다고 생각하자.</li>
</ul>
<p>$$y = \textbf{w}^T \textbf{x}$$</p>
<p>이렇게 할 수 있다. 하지만 overlapping되니까 class seperation을 최대화하는 projection을 하는 것이다.</p>
<ul>
<li>각 class의 평균을 $\textbf{m}_1, \textbf{m}_2$이라고 하면 아래의 값을 최대로 하는 ${\bf w}$를 찾아야 한다.
<ul>
<li>${\bf m_1}=1 / N_1\sum_{n \in C_1}\textbf{x}_n$</li>
<li>${\bf m_2}=1 / N_2\sum_{n \in C_2}\textbf{x}_n$</li>
</ul>
</li>
</ul>
<p>$$m_2 - m_1 = \textbf{w}^T (\textbf{m}_1 - \textbf{m}_2),\quad where; m_k = \textbf{w}^T \textbf{m}_k$$</p>
<p>${\bf w}$를 계속 키우면 커지기 때문에 제약식 $\sum {\bf w}_i^2 = 1$을 두고 라그랑지로 풀면</p>
<p>$${\bf w} \propto (\textbf{m}_2 - \textbf{m}_1)$$</p>
<p>의 결론을 얻는다.</p>
<ul>
<li>이에 추가적으로 Fisher는 <strong>within class의 varinace를 최소화</strong> 하고자 했다. 반면에 <strong>between class의 variance는 최대화</strong> 한다.</li>
<li>class $C_k$의 within variance는
<ul>
<li>$y_n = {\bf w}^T {\bf x}_n$</li>
<li>$m_k = {\bf w}^T {\bf m}_k$</li>
</ul>
</li>
</ul>
<p>$$s_k^2=\sum_{n \in C_k}(y_n-m_k)^2$$</p>
<p>전체 class의 within variance는 $s_1^2+s_2^2$ 이를 통해</p>
<ul>
<li>Fisher criterion (ratio of the between-class variance to the within-class variance)은</li>
</ul>
<p>$$J(\textbf{w}) = \frac{(m_2 - m_1)^2}{s_1^2+s_2^2}$$</p>
<ul>
<li>Fisher criterion을 다시 쓰면</li>
</ul>
<p>$$J(\textbf{w}) = \frac{\textbf{w}^T {\bf S}_B \textbf{w}}{\textbf{w}^T {\bf S}_W \textbf{w}}$$</p>
<p>$${\bf S}_B = (\textbf{m}_2 - \textbf{m}_1)(\textbf{m}_2 - \textbf{m}_1)^T$$</p>
<p>이 값은 between-class covariance matrix이다.</p>
<p>$$\textbf{S} _ W = \sum_{n \in C_1} (\textbf{x} _ n - \textbf{m} _ 1)(\textbf{x} _ n - \textbf{m} _ 1)^T + \sum_{n \in C_2} ({\bf x}_n - \textbf{m}_2)({\bf x}_n-\textbf{m}_2)^T$$</p>
<p>이 값은 total within-class covariance matrix이다.</p>
<ul>
<li>w에 대해 미분하고 위의 값을 최대화하는 값을 찾으면 $\textbf{w} \propto {\bf S}_W^{-1} (\textbf{m}_2 - \textbf{m}_1)$ . 이 결과를 <strong>Fisher&rsquo;s linear discriminant</strong> 라고 한다. 1차원에 projection한 뒤에 특정 threshold값을 정해 classification할 수 있다.</li>
</ul>
<h3 id="415-relation-to-least-squares">4.1.5 Relation to least squares</h3>
<p>Fisher criterion은 least square의 특별한 경우이다. target을 1-of-K encoding의 방법이 아닌</p>
<ul>
<li>class 1은 $N / N_1$</li>
<li>class 2는 $-N / N_2$</li>
</ul>
<p>으로 encoding 하면 된다. 이렇게 한 뒤에 least square의 방법대로 parameter를 구하면 Fisher criterion이 나온다. (과정은 생략)</p>
<h3 id="416-fishers-discriminant-for-multiple-classes">4.1.6 Fisher&rsquo;s discriminant for multiple classes</h3>
<ul>
<li>skip</li>
</ul>
<h3 id="417-the-perceptron-algorithm">4.1.7 The perceptron algorithm</h3>
<ul>
<li>perceptron 특징
<ul>
<li>2 class에서만 사용가능하다.</li>
<li>based on linear combination of fixed basis function</li>
<li>target을 이전에는 주로 1,0 으로 했는데 여기서는 -1, 1 로 코딩한다.</li>
</ul>
</li>
<li><strong>perceptron criterion</strong> (error function)</li>
</ul>
<p>$$E_p ({\bf w}) = -\sum_{n \in M}{\textbf{w}^T \phi_n t_n}$$</p>
<p>$M$은 잘못분류한 케이스를 의미한다. 우리는 이 criterion을 최소화 하고자 한다.</p>
<ul>
<li>
<p>$\textbf{w}^T \phi_n &gt; 0$ 이면 1로 분류</p>
</li>
<li>
<p>$\textbf{w}^T \phi_n &lt; 0$ 이면 -1로 분류</p>
<ul>
<li>따라서 분류를 잘못하면 ${\textbf{w}^T \phi_n t_n} &lt; 0$ 이고 error가 커지는 것이다.</li>
</ul>
</li>
<li>
<p>위 perceptron criterion을 SGD로 iterative하게 계산하면</p>
</li>
</ul>
<p>$${\bf w}^{(\tau+1)}={\bf w}^{(\tau)}-\eta\triangledown E_p({\bf w})={\bf w}^{(\tau)}+\eta\phi_n{t_n}$$</p>
<p>($\eta$는 learning rate) 이다. 이를 쉽게 해석하면 분류가 맞으면 놔두고 틀리면 그 $\phi_n$ 만큼 더하고 빼고 하는 것이다. 양변에 $-\phi_n t_n$을 곱하면 에러가 줄어듬(parameter가 converge)을 알 수 있다.</p>
<p>$$-{\bf w}^{(\tau+1)T}{\phi}_n{t_n} = -{\bf w}^{(\tau)T}{\phi_n}{t_n}-(\phi_n{t_n})^T\phi_n{t_n} &lt; -{\bf w}^{(\tau)T}\phi_n{t_n}$$</p>
<ul>
<li><em>perceptron convergence theorem</em>
<ul>
<li>training data set is linearly separable 하면 perceptron algorithm수렴한다 (반드시 해당하는 decision boundary를 찾을 수 있다) . 아니면 수렴이 안된다.</li>
<li>수렴하기 전까지 이게 non separable 문제인지 아니면 수렴이 천천히 되는 건지 파악하기 어렵다.</li>
</ul>
</li>
</ul>
<h2 id="42-probabilistic-generative-models">4.2 Probabilistic Generative Models</h2>
<p>data의 분포에 대한 가정을 갖는 decision boundary에 대해 공부해보자. $p(x|C_k), p(C_k)$로 베이즈정리를 이용하여 posterior를 계산한다. (일단 binary classification의 경우)</p>
<ul>
<li>posterior probability for class 1 :</li>
</ul>
<p>$$p(C_1 | {\bf x}) = \frac{p({\bf x}|C_1)p(C_1)}{p({\bf x}|C_1)p(C_1)+p({\bf x}|C_2)p(C_2)}$$</p>
<p>$$ = \frac{1}{1+\frac{p({\bf x}|C_2)p(C_2)}{p({\bf x}|C_1)p(C_1)}} = \frac{1}{1+exp(-a) } = \sigma (a)$$</p>
<p>$$\text{where}\; a = \ln \frac{p(x|C_1)p(C_1)}{p(x|C_2)p(C_2)}$$</p>
<ul>
<li>$\sigma (x) =  \frac{1}{1+exp(-x) }$ 이 식은 <em>logistic sigmoid</em> function 이다.</li>
<li>이의 inverse는 $x=\ln (\frac{\sigma}{1-\sigma})$ 이고 <em>logit</em> function이라고 한다.</li>
</ul>
<p>이번에는 일반적인 경우에 대해 살펴보자. multi class의 경우</p>
<p>$$p(C_k | {\bf x}) = \frac{p({\bf x}|C_k)p(C_k)}{\sum p({\bf x}|C_j)p(C_j)} = \frac{exp(a_k)}{\sum_j exp(a_j)}$$</p>
<p>$$\text{where}\; a_k = \ln p({\bf x}|C_k)p(C_k)$$</p>
<p>이를 <em>normalized exponential</em> or <em>softmax function</em> 이라고 한다.</p>
<h3 id="421-continuous-inputs">4.2.1 Continuous inputs</h3>
<p>class-conditional density를 Gaussian이라고 가정하고 posterior를 살펴보자. 단 모든 class는 같은 covariance matrix를 가진다. (2-class)</p>
<p>$$p({\bf x}|C_k) = \dfrac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}\exp \{-\dfrac{1}{2}({\bf x} - {\pmb \mu}_k)^T\Sigma^{-1}({\bf x} - {\pmb \mu}_k)\} $$</p>
<p>이므로 이를 이용해 위에서 구한 posterior를 계산하면</p>
<ul>
<li>$a = \ln \frac{p(x|C_1)p(C_1)}{p(x|C_2)p(C_2)}$</li>
</ul>
<p>$$p(C_1 | {\bf x}) =\sigma (a) =  \sigma ({\bf w}^T {\bf x} + w_0)$$</p>
<p>$${\bf w} = \Sigma^{-1}({\pmb \mu_1}-{\pmb \mu_2})$$</p>
<p>$$w_0 = -\frac{1}{2}{\pmb \mu_1}^T\Sigma^{-1}{\pmb \mu_1} + \frac{1}{2}{\pmb \mu_2}^T\Sigma^{-1}{\pmb\mu_2} + \ln{\frac{p(C_1)}{p(C_2)}}$$</p>
<p>의 형태가 나온다. class-conditional density를 Gaussian이라고 가정하였기 때문에 logistic sigmoid안에서 ${\bf x}$ 의 linear function의 형태가 나온다.</p>
<ul>
<li>K-class의 경우</li>
</ul>
<p>$$a_k({\bf x})=\ln(p({\bf x}|C_k)p(C_k)) = {\bf w}^T_k {\bf x}+w_0$$</p>
<p>$${\bf w}_k = \Sigma^{-1}{\pmb \mu}_k$$</p>
<p>$$w_{k0} = -\frac{1}{2}{\pmb \mu}_{k}^{T} \Sigma^{-1}{\pmb \mu}_k + \ln p(C_k)$$</p>
<ul>
<li>posterior의 decision boundary는 input space에 linear하다. (공분산이 동일하다는 가정하에서)</li>
<li>공분산을 각 class마다 다르다고 가정하면 우리는 quadratic function of ${\bf x}$를 얻게 되고 이는 <em>quadratic discriminant</em> 이다.</li>
</ul>
<p>이처럼 posterior probability는</p>
<p>$$p({\bf x}|C_k) = f(\text{linear of}\;{\bf x})$$</p>
<p>의 형태가 된다.</p>
<h3 id="422-maximum-likelihood-solution">4.2.2 Maximum likelihood solution</h3>
<p>MLE를 통해 prameter들을 추정해보자. class-conditional에서 Gaussian을 가정하였는데 그에 해당하는 parameter들 이다.</p>
<ul>
<li>prior : $p(C_1) = \pi , p(C_2) = 1- \pi$</li>
<li>$p(x_n,C_1) = p(C_1)p({\bf x}_n|C_1) = \pi N({\bf x}_n | {\pmb \mu}_1,{\pmb \Sigma})$</li>
<li>$p(x_n,C_2) = p(C_2)p({\bf x}_n|C_2) =(1- \pi) N({\bf x}_n | {\pmb \mu}_2,{\pmb \Sigma})$</li>
<li>Class 1은 1, Class 2는 0 으로 target coding</li>
<li>likelihood function :</li>
</ul>
<p>$$p(\textbf{t} | \pi, {\pmb \mu}_1,{ \pmb \mu}_2, {\pmb \Sigma} ) = \prod [\pi N({\bf x}_n | {\pmb \mu}_1, {\pmb \Sigma})]^{t_n}[(1-\pi)N({\bf x}_n | {\pmb \mu}_2, {\pmb \Sigma})]^{1-t_n}$$</p>
<p>이를 log 취하고 미분하여 MLE를 구하면 (K-class도 동일한 방법으로 구할 수 있다)</p>
<p>$$\pi = \frac{1}{N} \sum_{n=1}^{N}{t_n} = \frac{N_1}{N_1 + N_2}$$</p>
<p>$${\pmb \mu} _ 1 = \frac{1}{N_1} \sum_{n=1}^{N}t_n {\bf x} _ n, {\pmb \mu} _ 2 = \frac{1}{N_2}\sum_{n=1}^{N}(1-t_n){\bf x}_n$$</p>
<p>$${\pmb \Sigma} = {\bf S} = \frac{N_1}{N}{\bf S}_1 + \frac{N_2}{N}{\bf S}_2$$</p>
<h3 id="423-discrete-features">4.2.3 Discrete features</h3>
<p>각 input data feature가 2가지의 값을 갖는 discrete feature들이라고 가정해보자. 그러면 총 $2^D$의 경우 수가 생긴다. 이를 추정하기에는 너무 복잡하다. 따라서 naive bayes의 가정을 이용하면</p>
<p>$$p({\bf x}|C_k) = \prod_{i=1}^{D}\mu_{ki}^{x_i}(1-\mu_{ki})^{1-x_i} $$</p>
<p>$$a_k({\bf x})=\ln(p({\bf x}|C_k)p(C_k))$$</p>
<p>$$a_k({\bf x})=\sum_{i=1}^{D}\{x_i\ln \mu_{ki}+(1-x_i)\ln(1-\mu_{ki})\}+\ln p(C_k)$$</p>
<p>이 또한 linear한 형태이다.</p>
<h3 id="424-exponential-family">4.2.4 Exponential Family</h3>
<p>위에서 알 수 있듯이 input이 Gaussian이던 discrete이던지 우리에게 가장 중요한 posterior class probability는 generalized linear model과 sigmoid, softmax activation function에 의해 결정된다.</p>
<ul>
<li>이러한 특징은 class-conditional density가 exponential family의 경우 해당한다.</li>
</ul>
<p>$$p({\bf x};|\lambda_k) = h({\bf x})g(\lambda_k)\exp(\lambda_k^T u({\bf x})) $$</p>
<p>여기서 제약을 위한 parameter $s$를 추가하고 (잘 이해못함)</p>
<p>$$p({\bf x};|\lambda_k, s) = \dfrac{1}{s}h\left(\dfrac{1}{s}{\bf x}\right)g\left(\lambda_k\right)\exp \left(\dfrac{1}{s}\lambda_k^T u({\bf x})\right)$$</p>
<p>linear function을 구할 수 있다.</p>
<p>$$a({\bf x})=\dfrac{1}{s}(\lambda_1-\lambda_2)^T{\bf x}+\ln g(\lambda_1) - \ln g(\lambda_2) + \ln p(C_1) - \ln p(C_2)$$</p>
<p>$$a_k({\bf x}) = \dfrac{1}{s}\lambda_k^T{\bf x}+\ln g(\lambda_k) + \ln p(C_k)$$</p>
<ul>
<li>link function과 exp fam의 관계
<ul>
<li>EX) Bernoulli dist</li>
</ul>
</li>
</ul>
<p>$$L(\theta) = \prod \theta^{x_i}(1-\theta)^{1-x_i}= \exp {\sum{x_i \log \theta}+ \sum{(1-x_i)\log (1-\theta)} }
$$</p>
<p>$$=\exp { \sum{x_i} \log(\frac{\theta}{1-\theta})  }(1-\theta)^n$$</p>
<ul>
<li>위의 식에서 $\log (\frac{\theta}{1-\theta})$ 가 link function이다.</li>
<li>$\log (\frac{\theta}{1-\theta}) = \beta_0 + \beta_1 x_1+&hellip;+\beta_n x_n$ 이 이제 배울 logistic regression 이다.</li>
</ul></div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2021-11-26</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/prml-chap04-1/index.md" target="_blank">Read Markdown</a>
                        </span></div>
            <div class="post-info-share">
                <span></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw"></i>&nbsp;<a href="/tags/classification/">Classification</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/prml-chap03-2/" class="prev" rel="prev" title="[PRML] Chapter3 - Linear Models For Regression (2)"><i class="fas fa-angle-left fa-fw"></i>[PRML] Chapter3 - Linear Models For Regression (2)</a>
            <a href="/prml-chap04-2/" class="next" rel="next" title="[PRML] Chapter4 - Linear Models For Classification (2)">[PRML] Chapter4 - Linear Models For Classification (2)<i class="fas fa-angle-right fa-fw"></i></a></div>
</div>
</article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.101.0">Hugo</a> | Theme - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.10"><i class="far fa-kiss-wink-heart fa-fw"></i> LoveIt</a>
                </div><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2023</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="https://github.com/minsoo9506" target="_blank">minsoo9506</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/katex/copy-tex.min.css"><script type="text/javascript" src="/lib/smooth-scroll/smooth-scroll.min.js"></script><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/auto-render.min.js"></script><script type="text/javascript" src="/lib/katex/copy-tex.min.js"></script><script type="text/javascript" src="/lib/katex/mhchem.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":10},"comment":{},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","maxResultLength":10,"noResultsFound":"No results found","snippetLength":30,"type":"lunr"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
